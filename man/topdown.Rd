% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/topdown.R
\encoding{UTF-8}
\name{topdown}
\alias{topdown}
\title{Construct a top-down hierarchical classification prediction rule.}
\usage{
topdown(
  formula,
  data,
  dependent.variable.name,
  num.trees = 500,
  mtry,
  sample.fraction = 0.667,
  maxnthreads = TRUE
)
}
\arguments{
\item{formula}{Object of class \code{formula} or \code{character} describing the model to fit.}

\item{data}{Training data of class \code{data.frame}. See the 'Details' section for
the structure required for the outcome variable.}

\item{dependent.variable.name}{Name of outcome variable, needed if no formula given. For survival forests this is the time variable.}

\item{num.trees}{Number of trees used in the random forests. The default is 500.}

\item{mtry}{Number of variables to possibly split at in each node in the random forests. Default is the (rounded down) square root of the number variables.}

\item{sample.fraction}{Fraction of observations to sample. Default is 1 for sampling with replacement and 0.7 for sampling without replacement. For classification, this can be a vector of class-specific values.}

\item{maxnthreads}{Should the maximum number of threads available be used (\code{TRUE}) or should
the default parallelization from the package \code{mlr3} be used (\code{FALSE}).
Default is \code{TRUE}.}
}
\value{
Object of class \code{topdown} with elements
  \item{\code{modellist}}{This list contains the local classifiers, which are used
  in prediction through the function \code{\link{predict.topdown}}. It is a list
  of length equal to the number of nodes in the category tree, including the root node.
  Each list element is a list itself with two elements: \code{sub-classes} and \code{learner}.
  The first element, \code{sub-classes}, contains the sub-classes (or "direct descendants")
  of the node associated with the corresponding entry of \code{modellist}. For so-called
  leaf nodes, that is, classes with no descendants, \code{sub-classes} is NA. The second element,
  \code{learner}, provides the classifier object, which is a \code{mlr3} learner, containing
  a random forest prediction rule fitted by the \code{ranger} package. For leaf nodes, \code{learner}
  is NA because for leaf nodes no further classifications have to be performed.}
  \item{\code{allclasses}}{This is a factor variable, which contains all levels
  the predictions of test data can have. In prediction, we do not necessarily
  continue the top-down classification until the leaf nodes are reached, but
  the process is stopped as soon as the certainty of a classification is below
  a certain threshold (see \code{\link{predict.topdown}} for details).
  Therefore, classes can be predicted that did not occur in training. For example,
  for a human, we may predict 'animalia.chordata.mammalia.primates'
  instead of 'animalia.chordata.mammalia.primates.hominidae.homo.sapien'.
  We return the set of all possible class predictions \code{allclasses} because
  these are the factor levels assigned to the predictions in \code{\link{predict.topdown}}.}
}
\description{
Implements top-down hierarchical classification (see for example: Naik & Rangwala (2018)) with random forests as local classifiers.
Single-label data are supported and the outcome classes have to be organized
as a category tree. An example of a hierarchical classification problem of this
kind would be species categorization, where the kingdom would be broadest category,
followed by phylum, class, order, family,
genus, and species. Humans would be categorized as follows: 1) animalia (kingdom),
2) chordata (phylum), 3) mammalia (class), 4) primates (order), 5) hominidae (family),
6) homo (genus), 7) sapien (species). All this information has to be present for each observation
in the data via a single outcome variable in the \code{data.frame} provided via the
function \code{data}. The entries of this variable have to be structured in
a certain form, please see the 'Details' section below for explanation.
}
\details{
As stated above, the outcome variable provides the information on the outcome
class for each observation. Each entry of the outcome variable must start with
the broadest category, followed by a dot ("."), followed by the second-broadest
category, followed by a dot ("."), and so on. Consider, for example, the
species categorization described above. Here, an entry for a human would look
like this: 'animalia.chordata.mammalia.primates.hominidae.homo.sapien'.
In the above example, the categorization has depth 7. That is, there are
seven degrees of fineness (first degree "kingdom", second degree "phylum",
and so on). But the function can also deal with hierarchical classification
problems, for which the different branches of the category have different lengths
(or "depths"). For example, for one observation the categorization may have
depth 7, but for another observation the categorization (from a different branch
of the category tree) may have only depth 3.\cr
IMPORTANT: The names of the classes from all levels in the category tree need to be unique.
For example, it is not possible that hierarchy level 5 contains a class that
has the same name as a class from hierarchy level 3.\cr\cr
The outcome variable can be a factor (recommended) or a character.
The covariates (or "input features") can be categorical or numeric.
For training each local classifier, only the observations from the
category associated with the current node are used. For example, at the node
of class 'mammalia', only the observations from the class 'mammalia' are used
for learning a classifier that distinguishes between the sub-classes of
'mammalia' (e.g., 'primates').\cr
The random forests used as local classifiers perform probability predictions.
At the prediction stage, the sub-class with the largest predicted probability
is selected. If several classes have the maximum probability, a random
decision is made for one of these classes with maximum probability.
}
\examples{
\dontrun{

## Load package:

library("hierclass")


## Load the example data set 'datasim':

data(datasim)


## Set seed to make results reproducible:

set.seed(1234)


## Split data set into training and test data:

trainind <- sample(1:nrow(datasim), size=round((3/4)*nrow(datasim)))
datatrain <- datasim[trainind,]
datatest <- datasim[-trainind,]


## Construct a top-down hierarchical prediction rule using the training data:

object <- topdown(ydepvar ~ ., data=datatrain, num.trees=50)
# NOTE: In practice 'num.trees' should in general be larger
# to achieve better performance (default is 500).
# We use 50 trees here only for computational efficiency of
# the example.

object
}

}
\references{
\itemize{
  \item Naik, A., Rangwala, H. (2018). Large scale hierarchical classification: State of the art. Springer, Berlin, <\doi{10.1007/978-3-030-01620-3}>.
  }
}
\seealso{
\code{\link{predict.topdown}}
}
\author{
Roman Hornung
}
